{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Develop notebook\n",
    "Develop notebook for issue with staled training in scratch networks for Udacity [Deep Learning nanodegree](https://udacity.com/course/deep-learning-nanodegree--nd101), [dog-breed classifier](https://github.com/udacity/deep-learning-v2-pytorch/tree/master/project-dog-classification) project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from PIL import ImageFile\n",
    "# Set PIL to be tolerant of image files that are truncated.\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32        # Number of samples per batch in DataLoader\n",
    "num_workers = 4        # Number of DataLoder processes\n",
    "n_classes = 133        # Number of outcome classes. Used for network setup\n",
    "networks = {}          # Holds multiple networks to be trained\n",
    "checkpoints = {}       # Checkpoint name and locations\n",
    "TEST_IN_TRAIN = False   # If testing should be done in the training loop. Used for debuging/testing\n",
    "image_size = 224       # Network used image size\n",
    "\n",
    "## Train parameters\n",
    "n_epochs = 200               #\n",
    "loss_fn = \"cross_entropy\"    # Select loss function. Selects correct loss input preprocessing function for training and testing loops\n",
    "lr = 0.001                   # Learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "\n",
    "# Training transforms\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(20),\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "])\n",
    "# Same transform for test and validation sets\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "])\n",
    "\n",
    "# Data location>\n",
    "data_dir = \"dogImages/\"\n",
    "\n",
    "# Train, validation and test data location\n",
    "train_dir = os.path.join(data_dir,\"train/\")\n",
    "valid_dir = os.path.join(data_dir,\"valid/\")\n",
    "test_dir = os.path.join(data_dir,\"test/\")\n",
    "\n",
    "# Load and transform data\n",
    "train_data = datasets.ImageFolder(train_dir, transform=train_transforms)# + datasets.ImageFolder(train_dir, transform=train_transforms) + datasets.ImageFolder(train_dir, transform=train_transforms) # Increase sample threefold \n",
    "valid_data = datasets.ImageFolder(valid_dir, transform=test_transforms)\n",
    "test_data = datasets.ImageFolder(test_dir, transform=test_transforms)\n",
    "\n",
    "loaders_scratch = {\n",
    "    \"train\": torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=num_workers),\n",
    "    \"valid\": torch.utils.data.DataLoader(valid_data, batch_size=batch_size, shuffle=True, num_workers=num_workers),\n",
    "    \"test\" : torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions - `weight_init_normal`, `select_criterion`\n",
    "`weight_init_normal` (from [here](https://gist.github.com/jeasinema/ed9236ce743c8efaf30fa2ff732749f5)) sets network layer weigths to normally distributed, and is optional. `select_criterion` enables fast switching between different loss functions without having to modify network architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_init_normal(model):\n",
    "    '''\n",
    "    Initialize network weigths to normal random distribution.\n",
    "    From: https://gist.github.com/jeasinema/ed9236ce743c8efaf30fa2ff732749f5\n",
    "    \n",
    "    Args:\n",
    "        model: an initialize model\n",
    "    \n",
    "    Returns: None\n",
    "    \n",
    "    Usage:\n",
    "        model = Model()\n",
    "        model.apply(weight_init)\n",
    "    '''\n",
    "    if isinstance(model, nn.Conv2d):\n",
    "        init.xavier_normal_(model.weight.data)\n",
    "        if model.bias is not None:\n",
    "            init.normal_(model.bias.data)\n",
    "    elif isinstance(model, nn.Linear):\n",
    "        init.xavier_normal_(model.weight.data)\n",
    "        init.normal_(model.bias.data)\n",
    "    elif isinstance(model, nn.BatchNorm2d):\n",
    "        init.normal_(model.weight.data, mean=1, std=0.02)\n",
    "        init.constant_(model.bias.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_criterion(lossfn=\"cross_entropy\"):\n",
    "    '''\n",
    "    Helper function that selects valid loss and preprocessing (of output according to loss) functions. Some functions need\n",
    "    data that is preprocessed in various ways, e.g. NLLLoss expects LogSoftMax input, but CrossEntropyLoss includes LogSoftMax.\n",
    "    This selector simplifies changing loss function without having to rewrite the network arch, when testing various networks\n",
    "    and loss functions.\n",
    "    \n",
    "    Args:\n",
    "        fn: <fn> string with criterion name as in torch.nn.functional.<fn>\n",
    "    \n",
    "    Returns:\n",
    "        preprocess: preprocess function for CNN output before passed to criterion. Defaults is the linear identity function\n",
    "        Criterion: torch.nn.<Criterion> loss function corresponding\n",
    "    \n",
    "    Example:\n",
    "        >>> preprocess, criterion = select_criterion(\"nll_loss\")\n",
    "        >>> train(n_epochs, loaders_scratch, model, optimizer, \n",
    "                  criterion, use_cuda, checkpoints[network], lr_scheduler, preprocess)\n",
    "    '''\n",
    "    # Linear identity function by default\n",
    "    preprocess = lambda self : self\n",
    "    if lossfn == 'cross_entropy':\n",
    "        Criterion = nn.CrossEntropyLoss()\n",
    "    elif lossfn == 'mse_loss':\n",
    "        Criterion = nn.MSELoss()\n",
    "    elif lossfn == 'nll_loss':\n",
    "        Criterion = nn.NLLLoss()\n",
    "        # NLLLoss expects input to be LogSoftMax values\n",
    "        preprocess = lambda x : F.log_softmax(x, dim=1)\n",
    "    else:\n",
    "        print(fn + ' not recognized. Check input or add loss criterion to method. Selected default \"cross_entropy\".')\n",
    "        Criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    return Criterion, preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementations - `train`, `test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_epochs, loaders, model, optimizer, criterion=nn.CrossEntropyLoss(),\n",
    "                                               use_cuda=True,\n",
    "                                               save_path='model_train_default.pt',\n",
    "                                               lr_scheduler=None,\n",
    "                                               preprocess = lambda self: self):\n",
    "    \"\"\"\n",
    "    Model trainer.\n",
    "    \n",
    "    Args:\n",
    "        n_epochs: numer of training iterations of the complete training set\n",
    "        loaders: dataLoader with data for training\n",
    "        model: model to train\n",
    "        optimizer: backward propagation optimizer\n",
    "        criterion: training criterion\n",
    "        use_cuda: if cuda should be used for training\n",
    "        save_path: save path for trained model\n",
    "        lr_scheduler: learning rate scheduler for changing learning rates during training\n",
    "        preprocess: preprocess network output for criterion (for more interactive modeling)\n",
    "        \n",
    "    Returns: trained model\n",
    "    \"\"\" \n",
    "    # initialize tracker for minimum validation loss\n",
    "    valid_loss_min = np.Inf \n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        # initialize variables to monitor training and validation loss\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        \n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(loaders['train']):\n",
    "            # move to GPU if possible\n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            def closure():\n",
    "                '''\n",
    "                Compute the training error and gradient as input to optimizer. Required for \n",
    "                some optimizers (e.g. conjugate gradient and LBFGS).\n",
    "                '''\n",
    "                # Reset graditents for training batch\n",
    "                optimizer.zero_grad()\n",
    "                # Forward prop\n",
    "                output = model(data)\n",
    "                # preprocess criterion input\n",
    "                output = preprocess(output)\n",
    "                # Compute loss\n",
    "                loss = criterion(output,target)\n",
    "                # Backprop\n",
    "                loss.backward()\n",
    "                # Update global closure_loss - for training error\n",
    "                global closure_loss\n",
    "                closure_loss = loss\n",
    "                return loss\n",
    "            optimizer.step(closure)\n",
    "            # Train loss\n",
    "            train_loss = train_loss + ((1 / (batch_idx + 1)) * (closure_loss.data - train_loss))\n",
    "            \n",
    "        ######################    \n",
    "        # validate the model #\n",
    "        ######################\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (data, target) in enumerate(loaders['valid']):\n",
    "                # move to GPU if possible\n",
    "                if use_cuda:\n",
    "                    data, target = data.cuda(), target.cuda()\n",
    "                # forward pass\n",
    "                output = model(data)\n",
    "                # preprocess criterion input\n",
    "                output = preprocess(output)\n",
    "                # calculate loos\n",
    "                loss = criterion(output, target)\n",
    "                # Validation loss\n",
    "                valid_loss = valid_loss + ((1 / (batch_idx + 1)) * (closure_loss.data - valid_loss))\n",
    "\n",
    "        # print training/validation statistics \n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "            epoch, \n",
    "            train_loss,\n",
    "            valid_loss\n",
    "            ))\n",
    "        \n",
    "        ## Save the model if validation loss has decreased\n",
    "        if valid_loss < valid_loss_min:\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print('Current validation Loss: {:.6f} \\tPrevious min validation Loss: {:.6f}'.format(\n",
    "            valid_loss,\n",
    "            valid_loss_min\n",
    "            ))\n",
    "            valid_loss_min = valid_loss\n",
    "            print('Lowest validation score so far. Saving current model to: \\t ' + re.search('checkpoints/(.*)', save_path).group(1))\n",
    "            \n",
    "        # Learning rate scheduler decreases learning rate when learning stagnates. Optional\n",
    "        if lr_scheduler == None:\n",
    "            pass\n",
    "        else:\n",
    "            lr_scheduler.step(valid_loss_min)\n",
    "        \n",
    "        # Test to see training progress. Wastefull here. Only set to true for testing.\n",
    "        if TEST_IN_TRAIN == True:       # Global variable\n",
    "            if (epoch % 5 == 0):\n",
    "                print(' ################## \\n ## Test at ' + str(epoch) + ' epochs:  ...')\n",
    "                test(loaders, model, criterion, use_cuda)\n",
    "                print(' ################## ')\n",
    "            \n",
    "    # return trained model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(loaders, model, criterion=nn.CrossEntropyLoss(),\n",
    "                         use_cuda=True,\n",
    "                         preprocess = lambda self: self):\n",
    "    \"\"\"\n",
    "    Test model accuracy and print results.\n",
    "    \n",
    "    Args:\n",
    "        loaders: DataLoader with data for testing\n",
    "        model: model to test\n",
    "        criterion: test criterion for accuracy\n",
    "        use_cuda: if cuda should be used for training\n",
    "        preprocess: preprocess network output function for criterion (used with criterion switching)\n",
    "        \n",
    "    Returns: None\n",
    "    \"\"\"\n",
    "    # monitor test loss and accuracy\n",
    "    test_loss = 0.\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(loaders['test']):\n",
    "            # move to GPU if possible\n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            # forward pass\n",
    "            output = model(data)\n",
    "            # preprocess criterion input\n",
    "            output = preprocess(output)\n",
    "            # calculate loss\n",
    "            loss = criterion(output, target)\n",
    "            # update average test loss \n",
    "            test_loss = test_loss + ((1 / (batch_idx + 1)) * (loss.data - test_loss))\n",
    "            # convert output probabilities to predicted class\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            # compare predictions to true label\n",
    "            correct += np.sum(np.squeeze(pred.eq(target.data.view_as(pred))).cpu().numpy())\n",
    "            total += data.size(0)\n",
    "            \n",
    "    print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "    print('Test Accuracy: %2d%% (%2d/%2d)\\n' % (\n",
    "        100. * correct / total, correct, total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scratch CNN 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network for testing #VGG16 like model but simplified\n",
    "class Net_1(nn.Module):\n",
    "    def __init__(self, n_classes = 133): ## 133 classes in this problem\n",
    "        super(Net_1, self).__init__()\n",
    "        # Feature layers\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "            nn.Conv2d(128, 256, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "            nn.Conv2d(256, 512, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(4,4)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(25088,4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096,4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096,4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096,2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(2048,n_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ## Define forward behavior\n",
    "        x = self.features(x)\n",
    "        # Flatten input for classifier\n",
    "        x = x.view(-1, 25088)\n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "#-#-# You do NOT have to modify the code below this line. #-#-#\n",
    "\n",
    "# instantiate the CNN\n",
    "model_scratch = Net_1(n_classes)\n",
    "# normally distributed weights\n",
    "model_scratch.apply(weight_init_normal)\n",
    "\n",
    "networks['SNet1'] = model_scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scratch CNN 2\n",
    "Test of network and inspired by [Udacity Student Hub thread](https://study-hall.udacity.com/rooms/community:nd101:633452-project-300/community:thread-11891619222-594683?contextType=room) by user _Mahmoud H_ and code referenced in [Pastebin](https://pastebin.com/MBSxfqqy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_inputs = 6272*8\n",
    "\n",
    "# Simple network for testing. Shallower network that uses batch normalization\n",
    "class Net_2(nn.Module):\n",
    "    def __init__(self, n_classes = 133):  #n_classes = 133 for my problem\n",
    "        super(Net_2, self).__init__()\n",
    "        # Feature layers\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            \n",
    "            nn.Conv2d(16, 32, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            \n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "            nn.BatchNorm2d(64),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(linear_inputs,2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            #nn.Linear(500,500),\n",
    "            #nn.ReLU(),\n",
    "            #nn.Dropout(),\n",
    "            nn.Linear(2048,n_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Convolutions\n",
    "        x = self.features(x)\n",
    "        # Flatten input for classifier\n",
    "        x = x.view(-1, linear_inputs)\n",
    "        # Linear classifier\n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "#-#-# You do NOT have to modify the code below this line. #-#-#\n",
    "\n",
    "# instantiate the CNN\n",
    "model_scratch = Net_2(n_classes)\n",
    "# normally distributed weights\n",
    "#model_scratch.apply(weight_init_normal)\n",
    "\n",
    "networks['SNet2'] = model_scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Untrained VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGG16 model\n",
    "model_scratch = models.vgg16(pretrained=False)\n",
    "\n",
    "# Replicates VGG16 classifier, but changes outputs and adds nn.LogSoftMax for test loop compatibility\n",
    "classifier = nn.Sequential(\n",
    "    nn.Linear(in_features=25088, out_features=4096, bias=True),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(in_features=4096, out_features=4096, bias=True),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(in_features=4096, out_features=n_classes, bias=True)\n",
    "  )\n",
    "\n",
    "# Update classifier\n",
    "model_scratch.classifier = classifier\n",
    "# normally distributed weights\n",
    "model_scratch.apply(weight_init_normal)\n",
    "\n",
    "networks['VGG16'] = model_scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Untrained ResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet18\n",
    "model_scratch = models.resnet18(pretrained=False)\n",
    "\n",
    "from collections import OrderedDict\n",
    "classifier = nn.Sequential(OrderedDict([\n",
    "    ('fc1', nn.Linear(512, 512)),\n",
    "    ('relu1', nn.ReLU()),\n",
    "    ('drop1', nn.Dropout()),\n",
    "    ('fc2', nn.Linear(512, 200)),\n",
    "    ('relu2', nn.ReLU()),\n",
    "    ('drop2', nn.Dropout()),\n",
    "    ('fc3', nn.Linear(200,133))\n",
    "]))\n",
    "\n",
    "# Replace model last layer with this classifier\n",
    "model_scratch.apply(weight_init_normal)\n",
    "\n",
    "networks['ResNet18'] = model_scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train all untrained models sequentially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ########################################## \n",
      " #### \n",
      " ####  Working on SNet2 network. \n",
      " #### \n",
      " ########################################## \n",
      "\n",
      "Epoch: 1 \tTraining Loss: 5.671856 \tValidation Loss: 4.925028\n",
      "Current validation Loss: 4.925028 \tPrevious min validation Loss: inf\n",
      "Lowest validation score so far. Saving current model to: \t model_scratch_SNet2-cross_entropy-200_epochs.pt\n",
      "Epoch: 2 \tTraining Loss: 4.889029 \tValidation Loss: 4.990131\n",
      "Epoch: 3 \tTraining Loss: 4.852938 \tValidation Loss: 4.726913\n",
      "Current validation Loss: 4.726913 \tPrevious min validation Loss: 4.925028\n",
      "Lowest validation score so far. Saving current model to: \t model_scratch_SNet2-cross_entropy-200_epochs.pt\n",
      "Epoch: 4 \tTraining Loss: 4.852279 \tValidation Loss: 4.823976\n",
      "Epoch: 5 \tTraining Loss: 4.847648 \tValidation Loss: 4.892925\n",
      "Epoch: 6 \tTraining Loss: 4.828283 \tValidation Loss: 4.806569\n",
      "Epoch: 7 \tTraining Loss: 4.794207 \tValidation Loss: 5.101537\n",
      "Epoch: 8 \tTraining Loss: 4.770022 \tValidation Loss: 4.781450\n",
      "Epoch: 9 \tTraining Loss: 4.737456 \tValidation Loss: 4.739684\n",
      "Epoch: 10 \tTraining Loss: 4.725983 \tValidation Loss: 4.748209\n",
      "Epoch: 11 \tTraining Loss: 4.723496 \tValidation Loss: 4.411380\n",
      "Current validation Loss: 4.411380 \tPrevious min validation Loss: 4.726913\n",
      "Lowest validation score so far. Saving current model to: \t model_scratch_SNet2-cross_entropy-200_epochs.pt\n",
      "Epoch: 12 \tTraining Loss: 4.693279 \tValidation Loss: 4.523893\n",
      "Epoch: 13 \tTraining Loss: 4.689401 \tValidation Loss: 4.690566\n",
      "Epoch: 14 \tTraining Loss: 4.683591 \tValidation Loss: 4.879437\n",
      "Epoch: 15 \tTraining Loss: 4.662858 \tValidation Loss: 4.667114\n",
      "Epoch: 16 \tTraining Loss: 4.644825 \tValidation Loss: 4.524933\n",
      "Epoch: 17 \tTraining Loss: 4.654167 \tValidation Loss: 4.561290\n",
      "Epoch: 18 \tTraining Loss: 4.632915 \tValidation Loss: 4.674863\n",
      "Epoch: 19 \tTraining Loss: 4.618599 \tValidation Loss: 4.603031\n",
      "Epoch: 20 \tTraining Loss: 4.626437 \tValidation Loss: 4.509003\n",
      "Epoch: 21 \tTraining Loss: 4.613625 \tValidation Loss: 4.690514\n",
      "Epoch: 22 \tTraining Loss: 4.606194 \tValidation Loss: 4.507668\n",
      "Epoch: 23 \tTraining Loss: 4.607126 \tValidation Loss: 4.521229\n",
      "Epoch: 24 \tTraining Loss: 4.589334 \tValidation Loss: 5.038970\n",
      "Epoch: 25 \tTraining Loss: 4.574140 \tValidation Loss: 4.402253\n",
      "Current validation Loss: 4.402253 \tPrevious min validation Loss: 4.411380\n",
      "Lowest validation score so far. Saving current model to: \t model_scratch_SNet2-cross_entropy-200_epochs.pt\n",
      "Epoch: 26 \tTraining Loss: 4.578410 \tValidation Loss: 4.676081\n",
      "Epoch: 27 \tTraining Loss: 4.569178 \tValidation Loss: 4.321012\n",
      "Current validation Loss: 4.321012 \tPrevious min validation Loss: 4.402253\n",
      "Lowest validation score so far. Saving current model to: \t model_scratch_SNet2-cross_entropy-200_epochs.pt\n",
      "Epoch: 28 \tTraining Loss: 4.556940 \tValidation Loss: 4.822415\n",
      "Epoch: 29 \tTraining Loss: 4.542237 \tValidation Loss: 4.662426\n",
      "Epoch: 30 \tTraining Loss: 4.528179 \tValidation Loss: 4.788710\n",
      "Epoch: 31 \tTraining Loss: 4.510250 \tValidation Loss: 4.351327\n",
      "Epoch: 32 \tTraining Loss: 4.508194 \tValidation Loss: 4.863410\n",
      "Epoch: 33 \tTraining Loss: 4.500570 \tValidation Loss: 4.449986\n",
      "Epoch: 34 \tTraining Loss: 4.493718 \tValidation Loss: 4.232838\n",
      "Current validation Loss: 4.232838 \tPrevious min validation Loss: 4.321012\n",
      "Lowest validation score so far. Saving current model to: \t model_scratch_SNet2-cross_entropy-200_epochs.pt\n",
      "Epoch: 35 \tTraining Loss: 4.492423 \tValidation Loss: 4.586231\n",
      "Epoch: 36 \tTraining Loss: 4.472619 \tValidation Loss: 4.697655\n",
      "Epoch: 37 \tTraining Loss: 4.470284 \tValidation Loss: 4.571076\n",
      "Epoch: 38 \tTraining Loss: 4.455352 \tValidation Loss: 4.293369\n",
      "Epoch: 39 \tTraining Loss: 4.420696 \tValidation Loss: 4.290244\n",
      "Epoch: 40 \tTraining Loss: 4.427080 \tValidation Loss: 4.121648\n",
      "Current validation Loss: 4.121648 \tPrevious min validation Loss: 4.232838\n",
      "Lowest validation score so far. Saving current model to: \t model_scratch_SNet2-cross_entropy-200_epochs.pt\n",
      "Epoch: 41 \tTraining Loss: 4.419968 \tValidation Loss: 4.446424\n",
      "Epoch: 42 \tTraining Loss: 4.400007 \tValidation Loss: 4.387309\n",
      "Epoch: 43 \tTraining Loss: 4.399132 \tValidation Loss: 4.686599\n",
      "Epoch: 44 \tTraining Loss: 4.378520 \tValidation Loss: 4.534677\n",
      "Epoch: 45 \tTraining Loss: 4.378201 \tValidation Loss: 4.243519\n",
      "Epoch: 46 \tTraining Loss: 4.344778 \tValidation Loss: 4.059600\n",
      "Current validation Loss: 4.059600 \tPrevious min validation Loss: 4.121648\n",
      "Lowest validation score so far. Saving current model to: \t model_scratch_SNet2-cross_entropy-200_epochs.pt\n",
      "Epoch: 47 \tTraining Loss: 4.349491 \tValidation Loss: 4.643064\n",
      "Epoch: 48 \tTraining Loss: 4.365813 \tValidation Loss: 3.910842\n",
      "Current validation Loss: 3.910842 \tPrevious min validation Loss: 4.059600\n",
      "Lowest validation score so far. Saving current model to: \t model_scratch_SNet2-cross_entropy-200_epochs.pt\n",
      "Epoch: 49 \tTraining Loss: 4.336056 \tValidation Loss: 4.567540\n",
      "Epoch: 50 \tTraining Loss: 4.332584 \tValidation Loss: 4.354828\n",
      "Epoch: 51 \tTraining Loss: 4.345902 \tValidation Loss: 4.440300\n",
      "Epoch: 52 \tTraining Loss: 4.318657 \tValidation Loss: 4.178304\n",
      "Epoch: 53 \tTraining Loss: 4.314549 \tValidation Loss: 4.052769\n",
      "Epoch: 54 \tTraining Loss: 4.305791 \tValidation Loss: 4.124208\n",
      "Epoch: 55 \tTraining Loss: 4.295560 \tValidation Loss: 4.144619\n",
      "Epoch: 56 \tTraining Loss: 4.282155 \tValidation Loss: 4.002937\n",
      "Epoch: 57 \tTraining Loss: 4.272214 \tValidation Loss: 4.281631\n",
      "Epoch: 58 \tTraining Loss: 4.256543 \tValidation Loss: 4.057191\n",
      "Epoch: 59 \tTraining Loss: 4.261643 \tValidation Loss: 4.804079\n",
      "Epoch: 60 \tTraining Loss: 4.229010 \tValidation Loss: 4.185188\n",
      "Epoch: 61 \tTraining Loss: 4.233970 \tValidation Loss: 4.189951\n",
      "Epoch: 62 \tTraining Loss: 4.223321 \tValidation Loss: 4.478777\n",
      "Epoch: 63 \tTraining Loss: 4.205212 \tValidation Loss: 4.193234\n",
      "Epoch: 64 \tTraining Loss: 4.201169 \tValidation Loss: 4.301654\n",
      "Epoch: 65 \tTraining Loss: 4.207142 \tValidation Loss: 4.361188\n",
      "Epoch: 66 \tTraining Loss: 4.173365 \tValidation Loss: 4.259223\n",
      "Epoch: 67 \tTraining Loss: 4.163131 \tValidation Loss: 4.339850\n",
      "Epoch: 68 \tTraining Loss: 4.185033 \tValidation Loss: 4.195489\n",
      "Epoch: 69 \tTraining Loss: 4.168502 \tValidation Loss: 3.785373\n",
      "Current validation Loss: 3.785373 \tPrevious min validation Loss: 3.910842\n",
      "Lowest validation score so far. Saving current model to: \t model_scratch_SNet2-cross_entropy-200_epochs.pt\n",
      "Epoch: 70 \tTraining Loss: 4.147733 \tValidation Loss: 4.180027\n",
      "Epoch: 71 \tTraining Loss: 4.148800 \tValidation Loss: 4.166403\n",
      "Epoch: 72 \tTraining Loss: 4.130434 \tValidation Loss: 4.355034\n",
      "Epoch: 73 \tTraining Loss: 4.127433 \tValidation Loss: 4.198288\n",
      "Epoch: 74 \tTraining Loss: 4.114383 \tValidation Loss: 4.497705\n",
      "Epoch: 75 \tTraining Loss: 4.115338 \tValidation Loss: 3.774484\n",
      "Current validation Loss: 3.774484 \tPrevious min validation Loss: 3.785373\n",
      "Lowest validation score so far. Saving current model to: \t model_scratch_SNet2-cross_entropy-200_epochs.pt\n",
      "Epoch: 76 \tTraining Loss: 4.089261 \tValidation Loss: 4.062673\n",
      "Epoch: 77 \tTraining Loss: 4.093376 \tValidation Loss: 4.064183\n",
      "Epoch: 78 \tTraining Loss: 4.085120 \tValidation Loss: 4.085521\n",
      "Epoch: 79 \tTraining Loss: 4.089196 \tValidation Loss: 3.954568\n",
      "Epoch: 80 \tTraining Loss: 4.064450 \tValidation Loss: 3.983449\n",
      "Epoch: 81 \tTraining Loss: 4.071626 \tValidation Loss: 4.250549\n",
      "Epoch: 82 \tTraining Loss: 4.066406 \tValidation Loss: 3.912486\n",
      "Epoch: 83 \tTraining Loss: 4.034574 \tValidation Loss: 4.011352\n",
      "Epoch: 84 \tTraining Loss: 4.048766 \tValidation Loss: 4.377130\n",
      "Epoch: 85 \tTraining Loss: 4.017154 \tValidation Loss: 4.025543\n",
      "Epoch: 86 \tTraining Loss: 3.972083 \tValidation Loss: 3.820101\n",
      "Epoch: 87 \tTraining Loss: 4.010039 \tValidation Loss: 4.229727\n",
      "Epoch: 88 \tTraining Loss: 4.018764 \tValidation Loss: 4.226662\n",
      "Epoch: 89 \tTraining Loss: 3.991226 \tValidation Loss: 4.290118\n",
      "Epoch: 90 \tTraining Loss: 4.017407 \tValidation Loss: 3.651144\n",
      "Current validation Loss: 3.651144 \tPrevious min validation Loss: 3.774484\n",
      "Lowest validation score so far. Saving current model to: \t model_scratch_SNet2-cross_entropy-200_epochs.pt\n",
      "Epoch: 91 \tTraining Loss: 3.976985 \tValidation Loss: 3.687074\n",
      "Epoch: 92 \tTraining Loss: 3.975054 \tValidation Loss: 3.821623\n",
      "Epoch: 93 \tTraining Loss: 3.964356 \tValidation Loss: 4.069645\n",
      "Epoch: 94 \tTraining Loss: 3.967061 \tValidation Loss: 4.166665\n",
      "Epoch: 95 \tTraining Loss: 3.948912 \tValidation Loss: 3.674358\n",
      "Epoch: 96 \tTraining Loss: 3.930775 \tValidation Loss: 3.622437\n",
      "Current validation Loss: 3.622437 \tPrevious min validation Loss: 3.651144\n",
      "Lowest validation score so far. Saving current model to: \t model_scratch_SNet2-cross_entropy-200_epochs.pt\n",
      "Epoch: 97 \tTraining Loss: 3.958622 \tValidation Loss: 3.883593\n",
      "Epoch: 98 \tTraining Loss: 3.942751 \tValidation Loss: 3.781996\n",
      "Epoch: 99 \tTraining Loss: 3.922359 \tValidation Loss: 3.765344\n",
      "Epoch: 100 \tTraining Loss: 3.913095 \tValidation Loss: 3.769738\n",
      "Epoch: 101 \tTraining Loss: 3.907063 \tValidation Loss: 3.604156\n",
      "Current validation Loss: 3.604156 \tPrevious min validation Loss: 3.622437\n",
      "Lowest validation score so far. Saving current model to: \t model_scratch_SNet2-cross_entropy-200_epochs.pt\n",
      "Epoch: 102 \tTraining Loss: 3.875573 \tValidation Loss: 4.016201\n",
      "Epoch: 103 \tTraining Loss: 3.912820 \tValidation Loss: 4.411241\n",
      "Epoch: 104 \tTraining Loss: 3.905809 \tValidation Loss: 3.764061\n",
      "Epoch: 105 \tTraining Loss: 3.841291 \tValidation Loss: 3.349403\n",
      "Current validation Loss: 3.349403 \tPrevious min validation Loss: 3.604156\n",
      "Lowest validation score so far. Saving current model to: \t model_scratch_SNet2-cross_entropy-200_epochs.pt\n",
      "Epoch: 106 \tTraining Loss: 3.872028 \tValidation Loss: 3.710516\n",
      "Epoch: 107 \tTraining Loss: 3.847330 \tValidation Loss: 3.827832\n",
      "Epoch: 108 \tTraining Loss: 3.889852 \tValidation Loss: 4.017416\n",
      "Epoch: 109 \tTraining Loss: 3.829381 \tValidation Loss: 3.736190\n",
      "Epoch: 110 \tTraining Loss: 3.844110 \tValidation Loss: 3.793815\n",
      "Epoch: 111 \tTraining Loss: 3.851258 \tValidation Loss: 4.153242\n",
      "Epoch: 112 \tTraining Loss: 3.829976 \tValidation Loss: 4.094898\n",
      "Epoch: 113 \tTraining Loss: 3.813057 \tValidation Loss: 3.694678\n",
      "Epoch: 114 \tTraining Loss: 3.807017 \tValidation Loss: 3.746294\n",
      "Epoch: 115 \tTraining Loss: 3.828814 \tValidation Loss: 3.921717\n",
      "Epoch: 116 \tTraining Loss: 3.811117 \tValidation Loss: 3.992465\n",
      "Epoch: 117 \tTraining Loss: 3.795987 \tValidation Loss: 3.380019\n",
      "Epoch: 118 \tTraining Loss: 3.802243 \tValidation Loss: 3.296018\n",
      "Current validation Loss: 3.296018 \tPrevious min validation Loss: 3.349403\n",
      "Lowest validation score so far. Saving current model to: \t model_scratch_SNet2-cross_entropy-200_epochs.pt\n",
      "Epoch: 119 \tTraining Loss: 3.794808 \tValidation Loss: 3.628714\n",
      "Epoch: 120 \tTraining Loss: 3.796137 \tValidation Loss: 4.154953\n",
      "Epoch: 121 \tTraining Loss: 3.801595 \tValidation Loss: 3.708544\n",
      "Epoch: 122 \tTraining Loss: 3.744067 \tValidation Loss: 3.634346\n",
      "Epoch: 123 \tTraining Loss: 3.750241 \tValidation Loss: 4.059839\n",
      "Epoch: 124 \tTraining Loss: 3.752554 \tValidation Loss: 4.124684\n",
      "Epoch: 125 \tTraining Loss: 3.733461 \tValidation Loss: 4.116364\n",
      "Epoch: 126 \tTraining Loss: 3.747701 \tValidation Loss: 4.184982\n",
      "Epoch: 127 \tTraining Loss: 3.719661 \tValidation Loss: 3.368369\n",
      "Epoch: 128 \tTraining Loss: 3.736387 \tValidation Loss: 3.470699\n",
      "Epoch: 129 \tTraining Loss: 3.762049 \tValidation Loss: 3.746706\n",
      "Epoch: 130 \tTraining Loss: 3.733891 \tValidation Loss: 3.079425\n",
      "Current validation Loss: 3.079425 \tPrevious min validation Loss: 3.296018\n",
      "Lowest validation score so far. Saving current model to: \t model_scratch_SNet2-cross_entropy-200_epochs.pt\n",
      "Epoch: 131 \tTraining Loss: 3.753849 \tValidation Loss: 3.345002\n",
      "Epoch: 132 \tTraining Loss: 3.703487 \tValidation Loss: 3.843721\n",
      "Epoch: 133 \tTraining Loss: 3.736395 \tValidation Loss: 3.625770\n",
      "Epoch: 134 \tTraining Loss: 3.701264 \tValidation Loss: 3.843112\n",
      "Epoch: 135 \tTraining Loss: 3.671200 \tValidation Loss: 3.707821\n",
      "Epoch: 136 \tTraining Loss: 3.713005 \tValidation Loss: 3.476574\n",
      "Epoch: 137 \tTraining Loss: 3.691626 \tValidation Loss: 3.432756\n",
      "Epoch: 138 \tTraining Loss: 3.686047 \tValidation Loss: 3.740185\n",
      "Epoch: 139 \tTraining Loss: 3.681453 \tValidation Loss: 4.164299\n",
      "Epoch: 140 \tTraining Loss: 3.703361 \tValidation Loss: 3.125350\n",
      "Epoch: 141 \tTraining Loss: 3.672163 \tValidation Loss: 3.834144\n",
      "Epoch: 142 \tTraining Loss: 3.664109 \tValidation Loss: 4.103703\n",
      "Epoch: 143 \tTraining Loss: 3.660083 \tValidation Loss: 3.404737\n",
      "Epoch: 144 \tTraining Loss: 3.694082 \tValidation Loss: 3.516252\n",
      "Epoch: 145 \tTraining Loss: 3.683436 \tValidation Loss: 3.966622\n",
      "Epoch: 146 \tTraining Loss: 3.683702 \tValidation Loss: 3.650655\n",
      "Epoch: 147 \tTraining Loss: 3.653793 \tValidation Loss: 3.930813\n",
      "Epoch: 148 \tTraining Loss: 3.639248 \tValidation Loss: 3.295877\n",
      "Epoch: 149 \tTraining Loss: 3.638097 \tValidation Loss: 3.814458\n",
      "Epoch: 150 \tTraining Loss: 3.663610 \tValidation Loss: 3.601123\n",
      "Epoch: 151 \tTraining Loss: 3.669610 \tValidation Loss: 3.703093\n",
      "Epoch: 152 \tTraining Loss: 3.659623 \tValidation Loss: 3.306281\n",
      "Epoch: 153 \tTraining Loss: 3.647261 \tValidation Loss: 3.453186\n",
      "Epoch: 154 \tTraining Loss: 3.623184 \tValidation Loss: 3.796121\n",
      "Epoch: 155 \tTraining Loss: 3.618634 \tValidation Loss: 4.346075\n",
      "Epoch: 156 \tTraining Loss: 3.619298 \tValidation Loss: 4.414460\n",
      "Epoch: 157 \tTraining Loss: 3.622422 \tValidation Loss: 3.351806\n",
      "Epoch: 158 \tTraining Loss: 3.617014 \tValidation Loss: 3.969349\n",
      "Epoch: 159 \tTraining Loss: 3.646171 \tValidation Loss: 4.177705\n",
      "Epoch: 160 \tTraining Loss: 3.614146 \tValidation Loss: 3.891500\n",
      "Epoch: 161 \tTraining Loss: 3.628499 \tValidation Loss: 4.689837\n",
      "Epoch: 162 \tTraining Loss: 3.601746 \tValidation Loss: 3.452348\n",
      "Epoch: 163 \tTraining Loss: 3.613284 \tValidation Loss: 4.021716\n",
      "Epoch: 164 \tTraining Loss: 3.635668 \tValidation Loss: 4.012368\n",
      "Epoch: 165 \tTraining Loss: 3.622029 \tValidation Loss: 3.767512\n",
      "Epoch: 166 \tTraining Loss: 3.635274 \tValidation Loss: 3.517915\n",
      "Epoch: 167 \tTraining Loss: 3.624090 \tValidation Loss: 3.505170\n",
      "Epoch: 168 \tTraining Loss: 3.609214 \tValidation Loss: 3.722442\n",
      "Epoch: 169 \tTraining Loss: 3.595178 \tValidation Loss: 3.841122\n",
      "Epoch: 170 \tTraining Loss: 3.599088 \tValidation Loss: 3.529262\n",
      "Epoch: 171 \tTraining Loss: 3.598182 \tValidation Loss: 3.680466\n",
      "Epoch: 172 \tTraining Loss: 3.578248 \tValidation Loss: 3.566718\n",
      "Epoch: 173 \tTraining Loss: 3.585881 \tValidation Loss: 3.473976\n",
      "Epoch: 174 \tTraining Loss: 3.572491 \tValidation Loss: 3.609927\n",
      "Epoch: 175 \tTraining Loss: 3.586253 \tValidation Loss: 3.592353\n",
      "Epoch: 176 \tTraining Loss: 3.583544 \tValidation Loss: 4.064957\n",
      "Epoch: 177 \tTraining Loss: 3.585731 \tValidation Loss: 3.034849\n",
      "Current validation Loss: 3.034849 \tPrevious min validation Loss: 3.079425\n",
      "Lowest validation score so far. Saving current model to: \t model_scratch_SNet2-cross_entropy-200_epochs.pt\n",
      "Epoch: 178 \tTraining Loss: 3.552812 \tValidation Loss: 3.478863\n",
      "Epoch: 179 \tTraining Loss: 3.563738 \tValidation Loss: 3.437192\n",
      "Epoch: 180 \tTraining Loss: 3.575476 \tValidation Loss: 3.361337\n",
      "Epoch: 181 \tTraining Loss: 3.540786 \tValidation Loss: 3.704948\n",
      "Epoch: 182 \tTraining Loss: 3.565779 \tValidation Loss: 4.070486\n",
      "Epoch: 183 \tTraining Loss: 3.578505 \tValidation Loss: 3.243335\n",
      "Epoch: 184 \tTraining Loss: 3.561957 \tValidation Loss: 3.617889\n",
      "Epoch: 185 \tTraining Loss: 3.519026 \tValidation Loss: 2.771376\n",
      "Current validation Loss: 2.771376 \tPrevious min validation Loss: 3.034849\n",
      "Lowest validation score so far. Saving current model to: \t model_scratch_SNet2-cross_entropy-200_epochs.pt\n",
      "Epoch: 186 \tTraining Loss: 3.576035 \tValidation Loss: 3.573802\n",
      "Epoch: 187 \tTraining Loss: 3.549540 \tValidation Loss: 3.393597\n",
      "Epoch: 188 \tTraining Loss: 3.574435 \tValidation Loss: 3.730961\n",
      "Epoch: 189 \tTraining Loss: 3.548807 \tValidation Loss: 3.306198\n",
      "Epoch: 190 \tTraining Loss: 3.593629 \tValidation Loss: 3.119148\n",
      "Epoch: 191 \tTraining Loss: 3.538921 \tValidation Loss: 3.265158\n",
      "Epoch: 192 \tTraining Loss: 3.554796 \tValidation Loss: 3.793511\n",
      "Epoch: 193 \tTraining Loss: 3.540812 \tValidation Loss: 3.572119\n",
      "Epoch: 194 \tTraining Loss: 3.586189 \tValidation Loss: 4.147056\n",
      "Epoch: 195 \tTraining Loss: 3.520662 \tValidation Loss: 2.913659\n",
      "Epoch: 196 \tTraining Loss: 3.531240 \tValidation Loss: 3.756718\n",
      "Epoch: 197 \tTraining Loss: 3.540428 \tValidation Loss: 2.786005\n",
      "Epoch: 198 \tTraining Loss: 3.541875 \tValidation Loss: 3.386294\n",
      "Epoch: 199 \tTraining Loss: 3.539703 \tValidation Loss: 3.782825\n",
      "Epoch: 200 \tTraining Loss: 3.566281 \tValidation Loss: 3.979394\n"
     ]
    }
   ],
   "source": [
    "# Train loop\n",
    "for network in networks:\n",
    "    print('\\n ########################################## \\n #### \\n ####  Working on ' + network + ' network. \\n #### \\n ########################################## \\n')\n",
    "    model = networks[network]\n",
    "    # Move model to GPU if possible\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "    \n",
    "    # Preprocess output and criterion functions\n",
    "    criterion, preprocess = select_criterion(loss_fn)\n",
    "    # Optimizer\n",
    "    optimizer = optim.Adam(model_scratch.parameters(), lr = lr)\n",
    "    # Change change learning rate if plateaus\n",
    "    lr_scheduler = None# optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', verbose=True)\n",
    "    \n",
    "    # Checkpoint for best model (validation) for this network an number of epochs\n",
    "    checkpoints[network] = \"checkpoints/model_scratch_\" + network + '-' + loss_fn +  '-' + str(n_epochs) + '_epochs.pt'\n",
    "    \n",
    "    model = train(n_epochs, loaders_scratch, model, optimizer, \n",
    "                  criterion, use_cuda, checkpoints[network], lr_scheduler, preprocess)\n",
    "    \n",
    "    # Clean up models\n",
    "    del model, preprocess, criterion, optimizer, lr_scheduler # Hopefully prevents CUDA from running out of memory all the time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test trained models sequentially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ########################################## \n",
      " ####  Test SNet2 network. \n",
      " ########################################## \n",
      "\n",
      "Test Loss: 3.218578\n",
      "\n",
      "Test Accuracy: 24% (205/836)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for network in networks:\n",
    "    print('\\n ########################################## \\n ####  Test ' + network + ' network. \\n ########################################## \\n')\n",
    "    model = networks[network]\n",
    "    # Get current checkpoint name\n",
    "    checkpoint = checkpoints[network]\n",
    "\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "    \n",
    "    # Get loss function used to train network\n",
    "    loss_fn = re.search('-(.*)-', checkpoint).group(1)\n",
    "    \n",
    "    # Initiate preprocess and criterion functions\n",
    "    criterion, preprocess = select_criterion(loss_fn)\n",
    "    # load the model that got the best validation accuracy\n",
    "    model.load_state_dict(torch.load(checkpoint))\n",
    "    \n",
    "    # test function\n",
    "    test(loaders_scratch, model, criterion, use_cuda, preprocess)\n",
    "    \n",
    "    # Hopefully will prevent CUDA from running out of memory all the time\n",
    "    del model, preprocess, criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and testing transfer learning - ResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer learning base model architecture \n",
    "model_transfer = models.resnet18(pretrained=True)\n",
    "\n",
    "# Freeze layers\n",
    "for param in model_transfer.parameters():\n",
    "    param.requires_grad = False\n",
    "# Create classifier for classifying dog breed data\n",
    "from collections import OrderedDict\n",
    "classifier = nn.Sequential(OrderedDict([\n",
    "    ('fc1', nn.Linear(512, 512)),\n",
    "    ('relu1', nn.ReLU()),\n",
    "    ('dropout1', nn.Dropout()),\n",
    "    ('fc2', nn.Linear(512,133)),\n",
    "]))\n",
    "\n",
    "# Replace model last layer with this classifier\n",
    "model_transfer.fc = classifier#.apply(weight_init_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 3.000970 \tValidation Loss: 1.552920\n",
      "Current validation Loss: 1.552920 \tPrevious min validation Loss: inf\n",
      "Lowest validation score so far. Saving current model to: \t model_transfer_pre_trained_ResNet18-cross_entropy-20_epochs.pt\n",
      "Epoch: 2 \tTraining Loss: 1.571214 \tValidation Loss: 1.121937\n",
      "Current validation Loss: 1.121937 \tPrevious min validation Loss: 1.552920\n",
      "Lowest validation score so far. Saving current model to: \t model_transfer_pre_trained_ResNet18-cross_entropy-20_epochs.pt\n",
      "Epoch: 3 \tTraining Loss: 1.353349 \tValidation Loss: 1.396341\n",
      "Epoch: 4 \tTraining Loss: 1.230539 \tValidation Loss: 1.531572\n",
      "Epoch: 5 \tTraining Loss: 1.159399 \tValidation Loss: 0.593398\n",
      "Current validation Loss: 0.593398 \tPrevious min validation Loss: 1.121937\n",
      "Lowest validation score so far. Saving current model to: \t model_transfer_pre_trained_ResNet18-cross_entropy-20_epochs.pt\n",
      " ################## \n",
      " ## Test at 5 epochs:  ...\n",
      "Test Loss: 0.677683\n",
      "\n",
      "Test Accuracy: 79% (666/836)\n",
      "\n",
      " ################## \n",
      "Epoch: 6 \tTraining Loss: 1.099154 \tValidation Loss: 1.356171\n",
      "Epoch: 7 \tTraining Loss: 1.097251 \tValidation Loss: 1.052335\n",
      "Epoch: 8 \tTraining Loss: 1.058097 \tValidation Loss: 1.435197\n",
      "Epoch: 9 \tTraining Loss: 1.106952 \tValidation Loss: 0.824348\n",
      "Epoch: 10 \tTraining Loss: 1.015815 \tValidation Loss: 1.177698\n",
      " ################## \n",
      " ## Test at 10 epochs:  ...\n",
      "Test Loss: 0.580999\n",
      "\n",
      "Test Accuracy: 80% (677/836)\n",
      "\n",
      " ################## \n",
      "Epoch: 11 \tTraining Loss: 0.996711 \tValidation Loss: 1.353442\n",
      "Epoch: 12 \tTraining Loss: 0.991352 \tValidation Loss: 1.054879\n",
      "Epoch: 13 \tTraining Loss: 0.934250 \tValidation Loss: 0.718628\n",
      "Epoch: 14 \tTraining Loss: 0.947333 \tValidation Loss: 0.825168\n",
      "Epoch: 15 \tTraining Loss: 0.964288 \tValidation Loss: 1.345757\n",
      " ################## \n",
      " ## Test at 15 epochs:  ...\n",
      "Test Loss: 0.623831\n",
      "\n",
      "Test Accuracy: 81% (679/836)\n",
      "\n",
      " ################## \n",
      "Epoch: 16 \tTraining Loss: 0.944150 \tValidation Loss: 0.906100\n",
      "Epoch    15: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch: 17 \tTraining Loss: 0.799322 \tValidation Loss: 0.558539\n",
      "Current validation Loss: 0.558539 \tPrevious min validation Loss: 0.593398\n",
      "Lowest validation score so far. Saving current model to: \t model_transfer_pre_trained_ResNet18-cross_entropy-20_epochs.pt\n",
      "Epoch: 18 \tTraining Loss: 0.759334 \tValidation Loss: 1.026298\n",
      "Epoch: 19 \tTraining Loss: 0.757857 \tValidation Loss: 0.785607\n",
      "Epoch: 20 \tTraining Loss: 0.748690 \tValidation Loss: 0.931704\n",
      " ################## \n",
      " ## Test at 20 epochs:  ...\n",
      "Test Loss: 0.500394\n",
      "\n",
      "Test Accuracy: 84% (704/836)\n",
      "\n",
      " ################## \n",
      "Test Loss: 0.509484\n",
      "\n",
      "Test Accuracy: 83% (695/836)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if use_cuda:\n",
    "    model_transfer = model_transfer.cuda()\n",
    "\n",
    "n_epochs = 20\n",
    "\n",
    "loaders_transfer = loaders_scratch\n",
    "# Loss and preprocess function (if any)\n",
    "criterion_transfer, preprocess = select_criterion(loss_fn)\n",
    "# Train only fc layers, other parameters are frozen\n",
    "optimizer_transfer = optim.Adam(model_transfer.fc.parameters(), lr=lr)\n",
    "# Modify learning rate on some criterion\n",
    "lr_scheduler_transfer = optim.lr_scheduler.ReduceLROnPlateau(optimizer_transfer, 'min', verbose=True)\n",
    "\n",
    "checkpoint = \"checkpoints/model_transfer_pre_trained_ResNet18\" + '-' + loss_fn +  '-' + str(n_epochs) + '_epochs.pt'\n",
    "\n",
    "# train the model\n",
    "model_transfer = train(n_epochs, loaders_transfer, model_transfer, optimizer_transfer, \n",
    "                       criterion_transfer, use_cuda, checkpoint, lr_scheduler_transfer, preprocess)\n",
    "\n",
    "# load the model that got the best validation accuracy\n",
    "model_transfer.load_state_dict(torch.load(checkpoint))\n",
    "\n",
    "# Test pretrained model\n",
    "test(loaders_transfer, model_transfer, criterion_transfer, use_cuda, preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run next notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%run Udacity_dogbreed_code.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
