{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Develop notebook\n",
    "Develop notebook for issue with staled training in scratch networks for Udacity [Deep Learning nanodegree](https://udacity.com/course/deep-learning-nanodegree--nd101), [dog-breed classifier](https://github.com/udacity/deep-learning-v2-pytorch/tree/master/project-dog-classification) project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from PIL import ImageFile\n",
    "# Set PIL to be tolerant of image files that are truncated.\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20        # Number of samples per batch in DataLoader\n",
    "num_workers = 4\n",
    "n_classes = 133        # Number of outcome classes. Used for network setup\n",
    "networks = {}          # Holds multiple networks to be trained\n",
    "TEST_IN_TRAIN = True  # If testing should be done in the training loop. Used for debuging/testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load filenames for human and dog images\n",
    "#human_files = np.array(glob(\"lfw/*/*\"))\n",
    "#dog_files = np.array(glob(\"dogImages/*/*/*\"))\n",
    "\n",
    "# Training transforms\n",
    "image_size = 224 # Network used image size\n",
    "train_transforms = transforms.Compose([transforms.Resize(image_size),\n",
    "                                       transforms.RandomCrop(image_size),\n",
    "                                       transforms.RandomRotation(10),\n",
    "                                       transforms.RandomHorizontalFlip(p=0.4),\n",
    "                                       transforms.RandomVerticalFlip(p=0.4),\n",
    "                                       transforms.RandomGrayscale(p=0.2),\n",
    "                                       transforms.RandomAffine(30),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize((0.485, 0.456, 0.406), \n",
    "                                             (0.229, 0.224, 0.225))])\n",
    "# Transform for test and validation sets\n",
    "test_transforms = transforms.Compose([transforms.Resize(image_size),\n",
    "                                      transforms.RandomCrop(image_size),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize((0.485, 0.456, 0.406), \n",
    "                                             (0.229, 0.224, 0.225))])\n",
    "\n",
    "\n",
    "# Check if CUDA is available\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "# Data location\n",
    "data_dir = \"dogImages/\"\n",
    "# Train, validation and test data location\n",
    "train_dir = os.path.join(data_dir,\"train/\")\n",
    "valid_dir = os.path.join(data_dir,\"valid/\")\n",
    "test_dir = os.path.join(data_dir,\"test/\")\n",
    "# Load and transform data\n",
    "train_data = datasets.ImageFolder(train_dir, transform=train_transforms) + datasets.ImageFolder(train_dir, transform=train_transforms) + datasets.ImageFolder(train_dir, transform=train_transforms) ## Poor accuracy. Increase sample and \n",
    "valid_data = datasets.ImageFolder(valid_dir, transform=test_transforms)\n",
    "test_data = datasets.ImageFolder(test_dir, transform=test_transforms)\n",
    "\n",
    "loaders_scratch = {}\n",
    "loaders_scratch[\"train\"] = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "loaders_scratch[\"valid\"] = torch.utils.data.DataLoader(valid_data, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "loaders_scratch[\"test\"] = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=True, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions for initialization and testing architectures - `weight_init_normal`, `select_criterion`\n",
    "`weight_init_normal` (from [here](https://gist.github.com/jeasinema/ed9236ce743c8efaf30fa2ff732749f5)) sets network layer weigths to normally distributed, and is optional. `select_criterion` enables fast switching between different loss functions without having to modify network archs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_init_normal(model):\n",
    "    '''\n",
    "    Initialize network weigths to normal random distribution.\n",
    "    From: https://gist.github.com/jeasinema/ed9236ce743c8efaf30fa2ff732749f5\n",
    "    \n",
    "    Args:\n",
    "        model: an initialize model\n",
    "        \n",
    "    Returns:\n",
    "        None\n",
    "    \n",
    "    Usage:\n",
    "        model = Model()\n",
    "        model.apply(weight_init)\n",
    "    '''\n",
    "    if isinstance(model, nn.Conv2d):\n",
    "        init.xavier_normal_(model.weight.data)\n",
    "        if model.bias is not None:\n",
    "            init.normal_(model.bias.data)\n",
    "    elif isinstance(model, nn.Linear):\n",
    "        init.xavier_normal_(model.weight.data)\n",
    "        init.normal_(model.bias.data)\n",
    "    elif isinstance(model, nn.BatchNorm2d):\n",
    "        init.normal_(model.weight.data, mean=1, std=0.02)\n",
    "        init.constant_(model.bias.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_criterion(lossfn=\"cross_entropy\"):\n",
    "    '''\n",
    "    Helper function that selects valid loss and preprocessing (of output according to loss) functions. Some functions need\n",
    "    data that is preprocessed in various ways, e.g. NLLLoss expects LogSoftMax input, but CrossEntropyLoss includes LogSoftMax.\n",
    "    This selector simplifies changing loss function without having to rewrite the network arch, when testing various networks\n",
    "    and loss functions.\n",
    "    \n",
    "    Args:\n",
    "        fn: <fn> string with criterion name as in torch.nn.functional.<fn>\n",
    "    \n",
    "    Returns:\n",
    "        preprocess: preprocess function for CNN output before passed to criterion. Defaults is the linear identity function\n",
    "        Criterion: torch.nn.<Criterion> loss function corresponding\n",
    "    \n",
    "    Example:\n",
    "        >>> preprocess, criterion = select_criterion(\"nll_loss\")\n",
    "        >>> train(n_epochs, loaders_scratch, model, optimizer, \n",
    "                  criterion, use_cuda, checkpoints[network], lr_scheduler, preprocess)\n",
    "    '''\n",
    "    # Linear identity function by default\n",
    "    preprocess = lambda self : self\n",
    "    if lossfn == 'cross_entropy':\n",
    "        Criterion = nn.CrossEntropyLoss()\n",
    "    elif lossfn == 'mse_loss':\n",
    "        Criterion = nn.MSELoss()\n",
    "    elif lossfn == 'nll_loss':\n",
    "        Criterion = nn.NLLLoss()\n",
    "        # NLLLoss expects input to be LogSoftMax values\n",
    "        preprocess = lambda x : F.log_softmax(x, dim=1)\n",
    "    else:\n",
    "        print(fn + ' not recognized. Check input or add loss criterion to method. Selected default \"cross_entropy\".')\n",
    "        Criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    return Criterion, preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementations - `train`, `test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_epochs, loaders, model, optimizer, criterion=nn.CrossEntropyLoss(),\n",
    "                                               use_cuda=True,\n",
    "                                               save_path='model_train_default.pt',\n",
    "                                               lr_scheduler=None,\n",
    "                                               preprocess = lambda self: self):\n",
    "    \"\"\"\n",
    "    Model trainer.\n",
    "    \n",
    "    Args:\n",
    "        n_epochs: numer of training iterations of the complete training set\n",
    "        loaders: dataLoader with data for training\n",
    "        model: model to train\n",
    "        optimizer: backward propagation optimizer\n",
    "        criterion: training criterion\n",
    "        use_cuda: if cuda should be used for training\n",
    "        save_path: save path for trained model\n",
    "        lr_scheduler: learning rate scheduler for changing learning rates during training\n",
    "        preprocess: preprocess network output for criterion (for more interactive modeling)\n",
    "        \n",
    "    Returns: trained model\n",
    "    \"\"\"\n",
    "    # initialize tracker for minimum validation loss\n",
    "    valid_loss_min = np.Inf \n",
    "    \n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        # initialize variables to monitor training and validation loss\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        \n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(loaders['train']):\n",
    "            # move to GPU \n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            # Reset graditents for training batch\n",
    "            optimizer.zero_grad()\n",
    "            # Forward prop\n",
    "            output = model(data)\n",
    "            # Preprocess data for criterion\n",
    "            output = preprocess(output)\n",
    "            # Compute loss\n",
    "            loss = criterion(output,target)\n",
    "            # Compute backprop\n",
    "            loss.backward()\n",
    "            # Take step\n",
    "            optimizer.step()\n",
    "            # Train loss\n",
    "            train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.data - train_loss))\n",
    "            \n",
    "        ######################    \n",
    "        # validate the model #\n",
    "        ######################\n",
    "        model.eval()\n",
    "        #with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(loaders['valid']):\n",
    "                # move to GPU\n",
    "                if use_cuda:\n",
    "                    data, target = data.cuda(), target.cuda()\n",
    "                ## update the average validation loss\n",
    "                output = preprocess(output)\n",
    "                output = model(data)\n",
    "                loss = criterion(output, target)\n",
    "                # Validation loss\n",
    "                valid_loss = valid_loss + ((1 / (batch_idx + 1)) * (loss.data - valid_loss))\n",
    "            \n",
    "        # print training/validation statistics \n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "            epoch, \n",
    "            train_loss,\n",
    "            valid_loss\n",
    "            ))\n",
    "        \n",
    "        ## Save the model if validation loss has decreased\n",
    "        if valid_loss < valid_loss_min:\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print('Current validation Loss: {:.6f} \\tPrevious min validation Loss: {:.6f}'.format(\n",
    "            valid_loss,\n",
    "            valid_loss_min\n",
    "            ))\n",
    "            valid_loss_min = valid_loss\n",
    "            print('Lowest validation score so far. Saving current model to: \\t ' + re.search('checkpoints/(.*)', save_path).group(1))\n",
    "            \n",
    "        # Learning rate scheduler decreases learning rate when learning stagnates. Optional\n",
    "        if lr_scheduler == None:\n",
    "            pass\n",
    "        else:\n",
    "            lr_scheduler.step(valid_loss_min)\n",
    "        \n",
    "        # Test to see training progress. Wastefull here. Only set to true for testing.\n",
    "        if TEST_IN_TRAIN == True:       # Global variable\n",
    "            if (epoch % 5 == 0):\n",
    "                print(' ################## \\n ## Test at ' + str(epoch) + ' epochs:  ...')\n",
    "                test(loaders, model, criterion, use_cuda)\n",
    "                print(' ################## ')\n",
    "            \n",
    "    # return trained model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(loaders, model, criterion=nn.CrossEntropyLoss(),\n",
    "                         use_cuda=True,\n",
    "                         preprocess = lambda self: self):\n",
    "    \"\"\"\n",
    "    Test model accuracy and print results.\n",
    "    \n",
    "    Args:\n",
    "        loaders: DataLoader with data for testing\n",
    "        model: model to test\n",
    "        criterion: test criterion for accuracy\n",
    "        use_cuda: if cuda should be used for training\n",
    "        preprocess: preprocess network output function for criterion (used with criterion switching)\n",
    "        \n",
    "    Returns: None\n",
    "    \"\"\"\n",
    "    # monitor test loss and accuracy\n",
    "    test_loss = 0.\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(loaders['test']):\n",
    "            # move to GPU\n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "            # calculate the loss\n",
    "            output = preprocess(output)\n",
    "            loss = criterion(output, target)\n",
    "            # update average test loss \n",
    "            test_loss = test_loss + ((1 / (batch_idx + 1)) * (loss.data - test_loss))\n",
    "            # convert output probabilities to predicted class\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            # compare predictions to true label\n",
    "            correct += np.sum(np.squeeze(pred.eq(target.data.view_as(pred))).cpu().numpy())\n",
    "            total += data.size(0)\n",
    "            \n",
    "    print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "    print('Test Accuracy: %2d%% (%2d/%2d)\\n' % (\n",
    "        100. * correct / total, correct, total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scratch CNN 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network for testing #VGG16 - based but simplified by dropping layers at the end of the network\n",
    "class Net_1(nn.Module):\n",
    "    def __init__(self, n_classes = 133): ## 133 classes in this problem\n",
    "        super(Net_1, self).__init__()\n",
    "        # Feature layers\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "            nn.Conv2d(128, 256, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "            nn.Conv2d(256, 512, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(4,4)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(25088,4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096,4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096,4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096,2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(2048,n_classes)\n",
    "            #nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ## Define forward behavior\n",
    "        x = self.features(x)\n",
    "        # Flatten input for classifier\n",
    "        x = x.view(-1, 25088)\n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "#-#-# You do NOT have to modify the code below this line. #-#-#\n",
    "\n",
    "# instantiate the CNN\n",
    "model_scratch = Net_1(n_classes)\n",
    "# normally distributed weights\n",
    "model_scratch.apply(weight_init_normal)\n",
    "\n",
    "networks['SNet1'] = model_scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scratch CNN 2\n",
    "Test of network and inspired by [Udacity Student Hub thread](https://study-hall.udacity.com/rooms/community:nd101:633452-project-300/community:thread-11891619222-594683?contextType=room) by user _Mahmoud H_ and code referenced in [Pastebin](https://pastebin.com/MBSxfqqy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple network for testing. Just for testing becuse I am getting no training... :(\n",
    "class Net_2(nn.Module):\n",
    "    def __init__(self, n_classes = 133):  #n_classes = 133 for my problem\n",
    "        super(Net_2, self).__init__()\n",
    "        # Feature layers\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Conv2d(128, 256, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.Conv2d(256, 512, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.Conv2d(512, 512, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.MaxPool2d(4,4)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(4608,500),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(500,500),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(500,n_classes)\n",
    "            #nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Convolutions\n",
    "        x = self.features(x)\n",
    "        # Flatten input for classifier\n",
    "        x = x.view(-1, 4608)\n",
    "        # Linear classifier\n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "#-#-# You do NOT have to modify the code below this line. #-#-#\n",
    "\n",
    "# instantiate the CNN\n",
    "model_scratch = Net_2(n_classes)\n",
    "# normally distributed weights\n",
    "model_scratch.apply(weight_init_normal)\n",
    "\n",
    "networks['SNet2'] = model_scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Untrained VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGG16 model\n",
    "model_scratch = models.vgg16(pretrained=False)\n",
    "\n",
    "# Replicates VGG16 classifier, but changes outputs and adds nn.LogSoftMax for test loop compatibility\n",
    "classifier = nn.Sequential(\n",
    "    nn.Linear(in_features=25088, out_features=4096, bias=True),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(in_features=4096, out_features=4096, bias=True),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(in_features=4096, out_features=n_classes, bias=True)\n",
    "    #nn.LogSoftmax(dim=1)\n",
    "  )\n",
    "\n",
    "# Update classifier\n",
    "model_scratch.classifier = classifier\n",
    "# normally distributed weights\n",
    "model_scratch.apply(weight_init_normal)\n",
    "\n",
    "networks['VGG16'] = model_scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Untrained ResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet18\n",
    "model_scratch = models.resnet18(pretrained=False)\n",
    "\n",
    "from collections import OrderedDict\n",
    "classifier = nn.Sequential(OrderedDict([\n",
    "    ('fc1', nn.Linear(512, 512)),\n",
    "    ('relu1', nn.ReLU()),\n",
    "    ('drop1', nn.Dropout()),\n",
    "    ('fc2', nn.Linear(512, 200)),\n",
    "    ('relu2', nn.ReLU()),\n",
    "    ('drop2', nn.Dropout()),\n",
    "    ('fc3', nn.Linear(200,133))\n",
    "    #('output', nn.LogSoftmax(dim=1))\n",
    "]))\n",
    "\n",
    "# Replace model last layer with this classifier\n",
    "model_scratch.apply(weight_init_normal)\n",
    "\n",
    "networks['ResNet18'] = model_scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train all untrained models sequentially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ########################################## \n",
      " #### \n",
      " ####  Working on SNet1 network. \n",
      " #### \n",
      " ########################################## \n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "TEST_IN_TRAIN = True  # If testing should be done in the training loop. Used for debuging/testing\n",
    "# Stores checkpoint locations\n",
    "checkpoints = {}\n",
    "# Select loss function and preprocess function for training and testing loops\n",
    "loss_fn = \"cross_entropy\"\n",
    "\n",
    "for network in networks:\n",
    "    print('\\n ########################################## \\n #### \\n ####  Working on ' + network + ' network. \\n #### \\n ########################################## \\n')\n",
    "    model = networks[network]\n",
    "    # Move model to GPU if possible\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "    \n",
    "    #optimizer_scratch = optim.SGD(model_scratch.parameters(), lr = 0.01, momentum = 0.9, weight_decay = 0.0005, nesterov=True)\n",
    "    #lr_scheduler_scratch = optim.lr_scheduler.ReduceLROnPlateau(optimizer_scratch, 'min', verbose=True)\n",
    "    #criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Preprocess output and criterion functions\n",
    "    criterion, preprocess = select_criterion(loss_fn)\n",
    "    # Optimizer\n",
    "    optimizer = optim.Adam(model_scratch.parameters(), lr = 0.01)\n",
    "    # Change change learning rate if plateaus\n",
    "    lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', verbose=True)\n",
    "    \n",
    "    # Checkpoint for best model (validation) for this network an number of epochs\n",
    "    checkpoints[network] = \"checkpoints/model_scratch_\" + network + '-' + loss_fn +  '-' + str(n_epochs) + '_epochs.pt'\n",
    "    \n",
    "    model = train(n_epochs, loaders_scratch, model, optimizer, \n",
    "                  criterion, use_cuda, checkpoints[network], lr_scheduler, preprocess)\n",
    "    \n",
    "    # Clean up models\n",
    "    del model, preprocess, criterion, optimizer, lr_scheduler # Hopefully prevents CUDA from running out of memory all the time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test trained models sequentially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for network in networks:\n",
    "    print('\\n ########################################## \\n ####  Test ' + network + ' network. \\n ########################################## \\n')\n",
    "    model = networks[network]\n",
    "    # Get current checkpoint name\n",
    "    checkpoint = checkpoints[network]\n",
    "\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "    \n",
    "    # Get loss function used to train network\n",
    "    loss_fn = re.search('-(.*)-', checkpoint).group(1)\n",
    "    \n",
    "    # Initiate preprocess and criterion functions\n",
    "    criterion, preprocess = select_criterion(loss_fn)\n",
    "    # load the model that got the best validation accuracy\n",
    "    model.load_state_dict(torch.load(checkpoint))\n",
    "    \n",
    "    # test function\n",
    "    test(loaders_scratch, model, criterion, use_cuda, preprocess)\n",
    "    \n",
    "    # Hopefully will prevent CUDA from running out of memory all the time\n",
    "    del model, preprocess, criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and testing transfer learning - ResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "# Transfer learning base model architecture \n",
    "model_transfer = models.resnet18(pretrained=True)\n",
    "\n",
    "# Freeze layers\n",
    "for param in model_transfer.parameters():\n",
    "    param.requires_grad = False\n",
    "# Create classifier for classifying dog breed data\n",
    "from collections import OrderedDict\n",
    "classifier = nn.Sequential(OrderedDict([\n",
    "    ('fc1', nn.Linear(512, 512)),\n",
    "    ('relu1', nn.ReLU()),\n",
    "    #('dropout1', nn.Dropout()),\n",
    "    ('fc2', nn.Linear(512,133)),\n",
    "    #('output1', nn.LogSoftmax(dim=1))\n",
    "]))\n",
    "\n",
    "# Replace model last layer with this classifier\n",
    "model_transfer.fc = classifier.apply(weight_init_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_cuda:\n",
    "    model_transfer = model_transfer.cuda()\n",
    "\n",
    "# See training progression iteratively\n",
    "TEST_IN_TRAIN = True\n",
    "n_epochs = 20\n",
    "\n",
    "loaders_transfer = loaders_scratch\n",
    "# criterion_transfer = nn.NLLLoss() and preprocess data with nn.LogSoftMax(dim=1)\n",
    "criterion_transfer, preprocess = select_criterion(loss_fn)\n",
    "# Train the fc parameters, other parameters are frozen\n",
    "optimizer_transfer = optim.Adam(model_transfer.fc.parameters(), lr=0.01)\n",
    "# Scheduler for decreasing learning rate on plateau\n",
    "lr_scheduler_transfer = optim.lr_scheduler.ReduceLROnPlateau(optimizer_transfer, 'min', verbose=True)\n",
    "\n",
    "checkpoint = \"checkpoints/model_transfer_pre_trained_ResNet18\" + '-' + loss_fn +  '-' + str(n_epochs) + '_epochs.pt'\n",
    "\n",
    "# train the model\n",
    "model_transfer = train(n_epochs, loaders_transfer, model_transfer, optimizer_transfer, \n",
    "                       criterion_transfer, use_cuda, checkpoint, lr_scheduler_transfer, preprocess)\n",
    "\n",
    "# load the model that got the best validation accuracy\n",
    "model_transfer.load_state_dict(torch.load(checkpoint))\n",
    "\n",
    "# Test pretrained model\n",
    "test(loaders_transfer, model_transfer, criterion_transfer, use_cuda, preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
